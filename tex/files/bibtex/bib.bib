@inproceedings{Wang2017,
 abstract = {We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics.},
 archivePrefix = {arXiv},
 arxivId = {1611.06455},
 author = {Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
 booktitle = {Proceedings of the International Joint Conference on Neural Networks},
 doi = {10.1109/IJCNN.2017.7966039},
 eprint = {1611.06455},
 isbn = {9781509061815},
 month = {jun},
 pages = {1578--1585},
 publisher = {Institute of Electrical and Electronics Engineers Inc.},
 title = {{Time series classification from scratch with deep neural networks: A strong baseline}},
 volume = {2017-May},
 year = {2017}
}
@article{IsmailFawaz2019,
 abstract = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
 archivePrefix = {arXiv},
 arxivId = {1809.04356},
 author = {{Ismail Fawaz}, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre Alain},
 doi = {10.1007/s10618-019-00619-1},
 eprint = {1809.04356},
 file = {:home/filipkr/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ismail Fawaz et al. - 2019 - Deep learning for time series classification a review.pdf:pdf},
 issn = {1573756X},
 journal = {Data Mining and Knowledge Discovery},
 keywords = {Classification,Deep learning,Review,Time series},
 month = {jul},
 number = {4},
 pages = {917--963},
 publisher = {Springer New York LLC},
 title = {{Deep learning for time series classification: a review}},
 volume = {33},
 year = {2019}
}
@inproceedings{Zhang2020,
 abstract = {While being the de facto standard coordinate representation in human pose estimation, heatmap is never systematically investigated in the literature, to our best knowledge. This work fills this gap by studying the coordinate representation with a particular focus on the heatmap. Interestingly, we found that the process of decoding the predicted heatmaps into the final joint coordinates in the original image space is surprisingly significant for human pose estimation performance, which nevertheless was not recognised before. In light of the discovered importance, we further probe the design limitations of the standard coordinate decoding method widely used by existing methods, and propose a more principled distribution-aware decoding method. Meanwhile, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating accurate heatmap distributions for unbiased model training. Taking the two together, we formulate a novel Distribution-Aware coordinate Representation of Keypoint (DARK) method. Serving as a model-agnostic plug-in, DARK significantly improves the performance of a variety of state-of-the-art human pose estimation models. Extensive experiments show that DARK yields the best results on two common benchmarks, MPII and COCO, consistently validating the usefulness and effectiveness of our novel coordinate representation idea.},
 archivePrefix = {arXiv},
 arxivId = {1910.06278},
 author = {Zhang, Feng and Zhu, Xiatian and Dai, Hanbin and Ye, Mao and Zhu, Ce},
 doi = {10.1109/cvpr42600.2020.00712},
 eprint = {1910.06278},
 mendeley-groups = {xjob},
 month = {aug},
 pages = {7091--7100},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 title = {{Distribution-Aware Coordinate Representation for Human Pose Estimation}},
 year = {2020}
}

@misc{mmpose,
title = {{MMP}ose - {O}pen{MML}ab {P}ose {E}stimation {T}oolbox and {B}enchmark},
url = {https://github.com/open-mmlab/mmpose}}

@inproceedings{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:home/filipkr/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2014 - Microsoft COCO Common objects in context.pdf:pdf},
issn = {16113349},
mendeley-groups = {xjob},
read-more={https://cocodataset.org/#home},
month = {may},
number = {PART 5},
pages = {740--755},
publisher = {Springer Verlag},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@inproceedings{Pavllo2019,
 abstract = {In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11{\%}, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at https://github.com/facebookresearch/VideoPose3D.},
 archivePrefix = {arXiv},
 arxivId = {1811.11742},
 author = {Pavllo, Dario and Feichtenhofer, Christoph and Grangier, David and Auli, Michael},
 booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
 doi = {10.1109/CVPR.2019.00794},
 eprint = {1811.11742},
 isbn = {9781728132938},
 issn = {10636919},
 keywords = {And Body Pose,Face,Gesture},
 mendeley-groups = {xjob},
 month = {jun},
 pages = {7745--7754},
 publisher = {IEEE Computer Society},
 title = {{3D human pose estimation in video with temporal convolutions and semi-supervised training}},
 volume = {2019-June},
 year = {2019}
}

@misc{fchollet2020-gradcam,
 title = "Grad-CAM class activation visualization",
 author = "Francois Chollet",
 howpublished = "\url{https://keras.io/examples/vision/grad_cam/}",
 year = 2020,
 note = "Accessed: 2020-11-20"}


@article{IsmailFawaz2020,
 abstract = {This paper brings deep learning at the forefront of research into time series classification (TSC). TSC is the area of machine learning tasked with the categorization (or labelling) of time series. The last few decades of work in this area have led to significant progress in the accuracy of classifiers, with the state of the art now represented by the HIVE-COTE algorithm. While extremely accurate, HIVE-COTE cannot be applied to many real-world datasets because of its high training time complexity in O(N2{\textperiodcentered} T4) for a dataset with N time series of length T. For example, it takes HIVE-COTE more than 8 days to learn from a small dataset with N= 1500 time series of short length T= 46. Meanwhile deep learning has received enormous attention because of its high accuracy and scalability. Recent approaches to deep learning for TSC have been scalable, but less accurate than HIVE-COTE. We introduce InceptionTime—an ensemble of deep Convolutional Neural Network models, inspired by the Inception-v4 architecture. Our experiments show that InceptionTime is on par with HIVE-COTE in terms of accuracy while being much more scalable: not only can it learn from 1500 time series in one hour but it can also learn from 8M time series in 13 h, a quantity of data that is fully out of reach of HIVE-COTE.},
 archivePrefix = {arXiv},
 arxivId = {1909.04939},
 author = {{Ismail Fawaz}, Hassan and Lucas, Benjamin and Forestier, Germain and Pelletier, Charlotte and Schmidt, Daniel F. and Weber, Jonathan and Webb, Geoffrey I. and Idoumghar, Lhassane and Muller, Pierre Alain and Petitjean, Fran{\c{c}}ois},
 doi = {10.1007/s10618-020-00710-y},
 eprint = {1909.04939},
 file = {:home/filipkr/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ismail Fawaz et al. - 2020 - InceptionTime Finding AlexNet for time series classification.pdf:pdf},
 issn = {1573756X},
 journal = {Data Mining and Knowledge Discovery},
 keywords = {Deep learning,Inception,Scalable model,Time series classification},
 mendeley-groups = {xjob},
 month = {nov},
 number = {6},
 pages = {1936--1962},
 publisher = {Springer},
 title = {{InceptionTime: Finding AlexNet for time series classification}},
 volume = {34},
 year = {2020}
}

@article{Fauvel2020,
abstract = {We present XCM, an eXplainable Convolutional neural network for Multivariate time series classification. XCM is a new compact convolutional neural network which extracts information relative to the observed variables and time directly from the input data. Thus, XCM architecture enables a good generalization ability on both small and large datasets, while allowing the full exploitation of a faithful post-hoc model-specific explainability method (Gradient-weighted Class Activation Mapping) by precisely identifying the observed variables and timestamps of the input data that are important for predictions. Our evaluation firstly shows that XCM outperforms the state-of-the-art multivariate time series classifiers on both the large and small public UEA datasets. Furthermore, following the illustration of the performance and explainability of XCM on a synthetic dataset, we present how XCM can outperform the current most accurate state-of-the-art algorithm on a real-world application while enhancing explainability by providing faithful and more informative explanations.},
archivePrefix = {arXiv},
arxivId = {2009.04796},
author = {Fauvel, Kevin and Lin, Tao and Masson, V{\'{e}}ronique and Fromont, {\'{E}}lisa and Termier, Alexandre},
eprint = {2009.04796},
file = {:home/filipkr/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fauvel et al. - 2020 - XCM An Explainable Convolutional Neural Network for Multivariate Time Series Classification.pdf:pdf},
mendeley-groups = {xjob},
month = {sep},
title = {{XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification}},
year = {2020}
}

@article{Nae2017,
 abstract = {STUDY DESIGN: Cross-sectional study. BACKGROUND: Visual rating of postural orientation during functional tasks may be a valuable tool to track rehabilitation progress following anterior cruciate ligament (ACL) injury. A valid test battery assessing postural orientation as a separate construct is lacking. OBJECTIVES: To evaluate measurement properties of a test battery to assess postural orientation in patients with ACL injury. METHODS: The content validity of functional tasks was assessed by expert focus group discussions. Fifty-one patients (45{\%} women) with ACL injury performed 9 functional tasks of varying difficulty. Interpretability, internal consistency, interrater reliability, and measurement error were assessed for segment-specific postural orientation errors (POEs), within-task POEs, and total POE score. Postural orientation errors were scored on video on an ordinal scale from 0 (no POEs) to 3 (major POEs). RESULTS: Stair ascent, deep squat, and crossover hop for distance were excluded in focus group discussions. Postural orientation errors in some tasks were excluded due to floor effects. The minisquat and drop jump were excluded due to poor internal consistency ($\alpha$≤.184). Interrater reliability values for segment-specific POEs and within-task POEs yielded fair to almost perfect agreement ($\kappa$ = 0.429-0.875) and almost perfect agreement for total POE score (intraclass correlation coefficient = 0.842), without systematic differences between raters. The smallest detectable changes were 0.7 and 5 points for groups and individuals, respectively. CONCLUSION: The final test battery (single-leg mini-squat, stair descent, forward lunge, singleleg hop for distance) of 4 POEs (foot pronation, medial knee-to-foot position, hip joint POEs, and trunk segment POEs) demonstrated good measurement properties in people with ACL injury.},
 author = {Nae, Jenny and Creaby, Mark W. and Nilsson, Gustav and Crossley, Kay M. and Ageberg, Eva},
 doi = {10.2519/jospt.2017.7270},
 file = {:home/filipkr/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nae et al. - 2017 - Measurement Properties of a Test Battery to Assess Postural Orientation During Functional Tasks in Patients Undergoi.pdf:pdf},
 issn = {0190-6011},
 journal = {Journal of Orthopaedic {\&} Sports Physical Therapy},
 keywords = {Lower extremity,Outcome assessment (health care),Performance-based measures,Postural control,Reproducibility of results},
 mendeley-groups = {xjob},
 month = {oct},
 number = {11},
 pages = {1--42},
 publisher = {Movement Science Media},
 title = {{Measurement Properties of a Test Battery to Assess Postural Orientation During Functional Tasks in Patients Undergoing ACL Injury Rehabilitation}},
 url = {http://www.jospt.org/doi/10.2519/jospt.2017.7270},
 volume = {47},
 year = {2017}
}

@inproceedings{Sun2019,
 abstract = {In this paper, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: The COCO keypoint detection dataset and the MPII Human Pose dataset. In addition, we show the superiority of our network in pose tracking on the PoseTrack dataset. The code and models have been publicly available at https://github.com/leoxiaobin/deep-high-resolution-net.pytorch.},
 archivePrefix = {arXiv},
 arxivId = {1902.09212},
 author = {Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
 booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
 doi = {10.1109/CVPR.2019.00584},
 eprint = {1902.09212},
 isbn = {9781728132938},
 issn = {10636919},
 keywords = {And Body Pose,Face,Gesture},
 mendeley-groups = {xjob},
 month = {jun},
 pages = {5686--5696},
 publisher = {IEEE Computer Society},
 title = {{Deep high-resolution representation learning for human pose estimation}},
 volume = {2019-June},
 year = {2019},
}

@phdthesis{Nae2020,
 title = {Is seeing just believing? Measurement properties of visual assessment of Postural Orientation Errors (POEs) in people with anterior cruciate ligament injury},
 abstract = {Rupture of the anterior cruciate ligament (ACL) is a common knee injury among young physically active populations. The injury results in impaired physical functions, such as joint instability, limitations in daily activities and sport-specific activities, and worse movement quality, e.g., altered postural orientation. Postural orientation is defined as the ability to maintain alignment between body segments, and undesirable postural orientation is suggested to be a risk factor for subsequent injury. The “gold standard” for measuring postural orientation is with three-dimensional motion analysis. However, there is a need for a systematic feasible approach to evaluate postural orientation in the clinical setting, such as with visual assessment. Therefore, the primary aim of this thesis was to develop and evaluate clinically feasible measures of postural orientation in participants with or without lower extremity injury. Secondary aims were to evaluate sex differences in postural orientation and the association between postural orientation and other measures of physical function and self-reported outcomes, in men and women undergoing rehabilitation after ACL reconstruction.One systematic review with meta-analysis was conducted to summarize measurement properties of visual assessment of postural orientation in healthy populations, and populations with lower extremity injury (paper I). Evaluation of measurement properties (i.e., face validity, interpretability, internal consistency, inter-rater reliability, and measurement error) of a test battery for visual assessment of postural orientation errors (POEs) in patients with ACL injury were reported in two cross-sectional studies (papers II–III). Sex differences in POE scores (i.e., total POE score, POE subscales activity of daily living (ADL) and sport, and segment-specific POEs across tasks) were investigated in one cross-sectional study (paper IV). In the same paper, the association between POE scores and hop performance and Patient-Reported Outcome Measures (PROMs) were evaluated, in men and women with ACL reconstruction, separately.This thesis shows that visual assessment of the segment-specific POE knee medial-to-foot position (KMFP) is associated with two-dimensional and three-dimensional kinematic variables, and shows moderate to almost perfect reliability for the KMFP in healthy populations. For other segment-specific POEs or for patients with lower extremity injury there were not enough studies to permit any synthesis. The evaluation of measurement properties (face validity, interpretability, and internal consistency) of visual assessment of POEs during a variety of functional tasks in patients with ACL injury, resulted in the final test battery of 5 functional tasks (single-leg mini squat, stair descending, forward lunge, singe-leg hop for distance, and side-hop) and 6 segment-specific POEs (foot pronation, KMFP, femur medial to shank, femoral valgus, deviation of pelvis in any plane, and deviation of trunk in any plane). Women demonstrated worse POE scores compared with men and worse POE scores were associated with worse hop performance in women (especially the POE subscale ADL), but not in men.The results from this thesis indicate that visual assessment of the segment-specific POE KMFP is valid and reliable in healthy populations. However, there is limited evidence of measurement properties for visual assessment of other segment-specific POEs, and in patients with lower extremity injuries. The test battery for visual assessment of POEs showed no floor or ceiling effects, high internal consistency, and good inter-rater reliability in patients with ACL injury. This indicates that visual assessment of POEs can be used in patients with ACL injury, both in research and in clinical practice. Furthermore, the results suggest that postural orientation should be evaluated separately for men and women, and that the POE subscale ADL could be used to help clinicians to decide when it is time to progress to jumping exercises during rehabilitation of ACL injuries.},
 keywords = {knee injury, Lower extremity, Anterior Cruciate Ligament, orientation/spatial, Postural orientation, Performance-based measures, Reproducibility of results, Hop performance, Patient reported outcome measures},
 author = {{Älmqvist Nae}, Jenny},
 year = {2020},
 month = {jun},
 language = {English},
 isbn = {978-91-7619-940-4},
 series = {Lund University, Faculty of Medicine Doctoral Dissertation Series},
 publisher = {Lund University, Faculty of Medicine},
 number = {2020:78},
 school = {Department of Health Sciences}
}


@book{Goodfellow2016,
 title={Deep Learning},
 author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
 publisher={MIT Press},
 note={\url{http://www.deeplearningbook.org}},
 year={2016}
}


@article{Chen2020,
 abstract = {Vision-based monocular human pose estimation, as one of the most fundamental and challenging problems in computer vision, aims to obtain posture of the human body from input images or video sequences. The recent developments of deep learning techniques have been brought significant progress and remarkable breakthroughs in the field of human pose estimation. This survey extensively reviews the recent deep learning-based 2D and 3D human pose estimation methods published since 2014. This paper summarizes the challenges, main frameworks, benchmark datasets, evaluation metrics, performance comparison, and discusses some promising future research directions.},
 archivePrefix = {arXiv},
 arxivId = {2006.01423},
 author = {Chen, Yucheng and Tian, Yingli and He, Mingyi},
 doi = {10.1016/j.cviu.2019.102897},
 eprint = {2006.01423},
 file = {:home/filipkr/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Tian, He - 2020 - Monocular Human Pose Estimation A Survey of Deep Learning-based Methods.pdf:pdf},
 journal = {Computer Vision and Image Understanding},
 keywords = {Deep learning,Human pose estimation,Survey},
 mendeley-groups = {xjob},
 month = {jun},
 publisher = {Academic Press Inc.},
 title = {{Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods}},
 url = {http://arxiv.org/abs/2006.01423 http://dx.doi.org/10.1016/j.cviu.2019.102897},
 volume = {192},
 year = {2020}
}


@inproceedings{Krizhevsky2012,
author    = {Alex Krizhevsky and
  Ilya Sutskever and
  Geoffrey E. Hinton},
editor    = {Peter L. Bartlett and
Fernando C. N. Pereira and
Christopher J. C. Burges and
L{\'{e}}on Bottou and
Kilian Q. Weinberger},
title     = {ImageNet Classification with Deep Convolutional Neural Networks},
pages     = {1106--1114},
year      = {2012},
url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},
timestamp = {Fri, 06 Mar 2020 16:56:56 +0100},
biburl    = {https://dblp.org/rec/conf/nips/KrizhevskySH12.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{Andriluka2014,  author={M. {Andriluka} and L. {Pishchulin} and P. {Gehler} and B. {Schiele}},  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition},   title={2D Human Pose Estimation: New Benchmark and State of the Art Analysis},   year={2014},  volume={},  number={},  pages={3686-3693},  doi={10.1109/CVPR.2014.471}}

@article{Wu2017,
abstract = {Significant progress has been achieved in Computer Vision by leveraging large-scale image datasets. However, large-scale datasets for complex Computer Vision tasks beyond classification are still limited. This paper proposed a large-scale dataset named AIC (AI Challenger) with three sub-datasets, human keypoint detection (HKD), large-scale attribute dataset (LAD) and image Chinese captioning (ICC). In this dataset, we annotate class labels (LAD), keypoint coordinate (HKD), bounding box (HKD and LAD), attribute (LAD) and caption (ICC). These rich annotations bridge the semantic gap between low-level images and high-level concepts. The proposed dataset is an effective benchmark to evaluate and improve different computational methods. In addition, for related tasks, others can also use our dataset as a new resource to pre-train their models.},
archivePrefix = {arXiv},
arxivId = {1711.06475},
author = {Wu, Jiahong and Zheng, He and Zhao, Bo and Li, Yixin and Yan, Baoming and Liang, Rui and Wang, Wenjia and Zhou, Shipei and Lin, Guosen and Fu, Yanwei and Wang, Yizhou and Wang, Yonggang},
eprint = {1711.06475},
journal = {arXiv},
month = {nov},
publisher = {arXiv},
title = {{AI Challenger : A Large-scale Dataset for Going Deeper in Image Understanding}},
url = {http://arxiv.org/abs/1711.06475},
year = {2017}
}


@article{Jin2020,
abstract = {This paper investigates the task of 2D human whole-body pose estimation, which aims to localize dense landmarks on the entire human body including face, hands, body, and feet. As existing datasets do not have whole-body annotations, previous methods have to assemble different deep models trained independently on different datasets of the human face, hand, and body, struggling with dataset biases and large model complexity. To fill in this blank, we introduce COCO-WholeBody which extends COCO dataset with whole-body annotations. To our best knowledge, it is the first benchmark that has manual annotations on the entire human body, including 133 dense landmarks with 68 on the face, 42 on hands and 23 on the body and feet. A single-network model, named ZoomNet, is devised to take into account the hierarchical structure of the full human body to solve the scale variation of different body parts of the same person. ZoomNet is able to significantly outperform existing methods on the proposed COCO-WholeBody dataset. Extensive experiments show that COCO-WholeBody not only can be used to train deep models from scratch for whole-body pose estimation but also can serve as a powerful pre-training dataset for many different tasks such as facial landmark detection and hand keypoint estimation. The dataset is publicly available at https://github.com/jin-s13/COCO-WholeBody.},
archivePrefix = {arXiv},
arxivId = {2007.11858},
author = {Jin, Sheng and Xu, Lumin and Xu, Jin and Wang, Can and Liu, Wentao and Qian, Chen and Ouyang, Wanli and Luo, Ping},
eprint = {2007.11858},
file = {:home/filipkr/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin et al. - 2020 - Whole-Body Human Pose Estimation in the Wild.pdf:pdf},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Facial landmark detection,Hand keypoint estimation,Whole-body human pose estimation},
mendeley-groups = {xjob},
month = {jul},
pages = {196--214},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Whole-Body Human Pose Estimation in the Wild}},
volume = {12354 LNCS},
year = {2020}
}


@article{Wang2019,
abstract = {We present a cascaded convolutional neural network for 2D hand pose estimation from single in-the-wild RGB images. Inspired by the commonly used silhouette information in the generative pose estimation approaches, we build the cascaded network with two stages, including mask prediction stage as well as pose estimation stage. We find that the two stages network architecture for end-to-end training could benefit from each other for detecting the hand mask and 2D pose. To further improve the hand pose detection accuracy, we contribute a new RGB hand dataset named OneHand10K, which contains 10K RGB images. Each image contains one single hand. We manually obtain the segmented mask and labeled keypoints for guided learning. We hope that this dataset will be a benchmark and encourage more people to conduct research on this challenging topic. Experiments on the validation dataset have demonstrated the superior performance of the proposed cascaded convolutional neural network.},
author = {Wang, Yangang and Peng, Cong and Liu, Yebin},
doi = {10.1109/TCSVT.2018.2879980},
issn = {15582205},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Hand pose estimation,cascaded CNN,mask prediction},
mendeley-groups = {xjob},
month = {nov},
number = {11},
pages = {3258--3268},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Mask-Pose Cascaded CNN for 2D Hand Pose Estimation from Single Color Image}},
volume = {29},
year = {2019}
}
