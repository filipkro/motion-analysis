% !TEX root=../../mt-motion-analysis.tex
\chapter{Related work - Time Series Classification} \label{sec:tsc}
Time series are sequences of data ordered in time [H'R KANSKE TA EN FIN DEFINITION FR[N ANDREAS...].

\begin{definition}
  \text{A univariate time series of length n, with ordered indices}
  $$X = \left[x_1, x_2, \hdots, x_n\right]^\intercal$$
  \label{def:uts}
\end{definition}

\begin{definition}
    \text{A multivariate time series of length n, with M channels}
    $$\pmb{X} = \left[X_1, \hdots, X_M\right]$$
\end{definition}

The \gls{tsc} task is about finding a function, $f: \mathbb{R}^{n \times M} \rightarrow \mathbb{R}$, that assigns one label to each, possibly multivariate, time series. The problem bares strong resemblance with that of image classification, but with the two spatial dimensions replaced by one temporal dimension. Despite this the use of end-to-end deep learning models is not as dominant in the \gls{tsc} community \cite{IsmailFawaz2019}. Similarily to the fields of computer vision various datasets has emerged recently. This has been important for the development of \gls{tsc} as it allows for fair comparison between methods. One of the most widely used dataset collections today is the \gls{ucr} archive \cite{Dau2018} containing 85 different time series datasets.

Traditionally a nearest neighbor method together with dynamic time warping has been used for classification \cite{Bagnall2017}. Simply put, this means that a time series during classification is compared to the training data and assigned the class of the most similar time series. Lines and Bagnall suggested a method where an ensemble of 11 nearest neighbor classifiers with different similarity measures \cite{Lines2015} yielded \gls{sota} results. Bagnall et al. \cite{Bagnall2015} developed the idea of ensemble based classifiers with \gls{cote}, where 35 different classifiers using different transforms was used. Lines et al. \cite{Lines2016} extended \gls{cote} further with two new classifiers resulting in \gls{hive-cote}. One drawback with \gls{hive-cote} is the computational intensity, both during training and test time. Training time is large partly due to one of the transforms used is the Shapelet Transform with a time complexity of $O(n^2l^4)$, $n$ being the number of time series and $l$ the length of them. Due to the nature of the nearest neighbor algorithm the result of the 37 classifiers during test time needs to be compared to the corresponding result for each time series in the training set, yielding this method impractical for real-time use \cite{IsmailFawaz2019}.

In 2016 Zheng et al. \cite{Zheng2016} presented a neural network model based on convolutional layers for the classification task. Wang et al. \cite{Wang2017} developed these ideas and presented models with performance close to that of \gls{cote} on the \gls{ucr} archive. The development of neural network based classification has since then continued and below the two architectures inspiring our model are presented.

\subsection{InceptionTime}
